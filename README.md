# Financial Forecasting System

A comprehensive system for scraping financial data, building predictive models, and forecasting future stock prices.

## ğŸ“‹ Overview

This system is comprised of three main components:

1. **Scraping Module**: Collects financial data from various sources
2. **Modeling Module**: Processes data and builds predictive models
3. **Forecasting Module**: Generates forecasts based on models and provides final output

The system is designed to be run as a complete pipeline or as individual components depending on your needs.

## ğŸ—ï¸ Project Structure

```
financial-forecasting/
â”œâ”€â”€ scraping/              # Financial data scraper
â”‚   â”œâ”€â”€ core/              # Core utilities
â”‚   â”œâ”€â”€ scrapers/          # Data scrapers
â”‚   â”œâ”€â”€ constants.py       # Configuration constants
â”‚   â”œâ”€â”€ main.py            # Main entry point
â”‚   â””â”€â”€ update_all.py      # Update existing data
â”œâ”€â”€ modelling/             # Time series modeling
â”‚   â”œâ”€â”€ models/            # Model implementations
â”‚   â”œâ”€â”€ utils/             # Utility functions
â”‚   â”œâ”€â”€ config/            # Configuration
â”‚   â”œâ”€â”€ predictions/       # Model predictions
â”‚   â””â”€â”€ update_predictions.py # Update prediction files
â”œâ”€â”€ forecasting/           # Final forecasting system
â”‚   â”œâ”€â”€ src/               # Source code
â”‚   â”œâ”€â”€ data/              # Data files
â”‚   â””â”€â”€ main.py            # Main entry point
â”œâ”€â”€ run_pipeline.py        # Complete pipeline orchestrator
â”œâ”€â”€ requirements.txt       # Consolidated dependencies
â””â”€â”€ Dockerfile             # Container definition
```

## ğŸš€ Quick Start

### Using Docker

1. Build the Docker image:
   ```bash
   docker build -t financial-forecasting .
   ```

2. Run the complete pipeline:
   ```bash
   docker run -v ./data:/app/scraping/scraped_data -e FRED_API_KEY=your_api_key financial-forecasting
   ```

### Manual Setup

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Create .env file with your API keys:
   ```
   FRED_API_KEY=your_fred_api_key_here
   ```

3. Run the complete pipeline:
   ```bash
   python run_pipeline.py
   ```

## âš™ï¸ Configuration

### Environment Variables

- `FRED_API_KEY`: API key for Federal Reserve Economic Data (required for market data)

### Command Line Options

The main pipeline script accepts the following arguments:

```bash
python run_pipeline.py [options]
```

Options:
- `--skip-scraping`: Skip the data scraping step
- `--skip-modeling`: Skip the modeling step
- `--skip-forecasting`: Skip the forecasting step
- `--log-level`: Set logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)

## ğŸ§© Components

### Scraping Module

The scraping module collects financial data from various sources including stock prices and market indicators.

Key features:
- Individual stock data collection
- Market-wide indicators collection
- Incremental updates
- Robust error handling

To run independently:
```bash
python -m scraping.update_all
```

### Modeling Module

The modeling module processes scraped data and builds predictive models using various time series forecasting techniques.

Supported models:
- SARIMA (Seasonal AutoRegressive Integrated Moving Average)
- AutoTS (Automated Time Series model selection)
- TimeMOE (Transformer-based time series forecasting)

To run independently:
```bash
python -m modelling.update_predictions
```

### Forecasting Module

The forecasting module combines model predictions with additional features to generate final forecasts.

Key features:
- Data preparation and feature engineering
- Model evaluation and selection
- Comprehensive error metrics
- JSON output for next week predictions

To run independently:
```bash
python -m forecasting.main
```

## ğŸ“Š Data Flow

1. **Scraping**: Raw financial data is collected and saved to `scraping/scraped_data/`
2. **Modeling**: Models are trained on scraped data and predictions are saved to `modelling/predictions/`
3. **Forecasting**: Final forecasts are generated using model predictions and saved to `forecasting/data/`

## ğŸ”„ Updating Existing Data

To update existing data without starting from scratch:

```bash
python run_pipeline.py
```

The system is designed to automatically detect existing data and update it with the latest information.

## ğŸ“ License

[MIT License](LICENSE)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
