# Financial Data Scraper - Refactored

A clean, modular financial data scraping system for collecting stock prices and market data.

## ğŸ—ï¸ Architecture

The refactored codebase follows clean architecture principles with clear separation of concerns:

```
scraping/
â”œâ”€â”€ constants.py              # Configuration constants
â”œâ”€â”€ main_refactored.py        # Main entry point
â”œâ”€â”€ update_all_refactored.py  # Data update script
â”œâ”€â”€ requirements_refactored.txt # Dependencies
â”œâ”€â”€ .env.template            # Environment variables template
â”œâ”€â”€ core/                    # Core utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py           # Centralized logging
â”‚   â”œâ”€â”€ file_manager.py     # File system operations
â”‚   â””â”€â”€ data_processor.py   # Data cleaning and processing
â””â”€â”€ scrapers/               # Data scrapers
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ base_scraper.py     # Base class with common functionality
    â”œâ”€â”€ company_scraper.py  # Individual stock data scraper
    â””â”€â”€ market_scraper.py   # Market-wide data scraper
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements_refactored.txt
```

### 2. Set Up Environment

```bash
cp .env.template .env
# Edit .env and add your FRED API key
```

### 3. Run the Scraper

```python
# Run all scrapers with default settings
python main_refactored.py

# Or use the orchestrator programmatically
from main_refactored import ScraperOrchestrator

orchestrator = ScraperOrchestrator(fred_api_key="your_key")
orchestrator.run_all_scrapers()
```

## ğŸ“Š Features

### Clean Code Principles Applied

- **Single Responsibility**: Each class has one clear purpose
- **DRY Principle**: Common functionality abstracted into base classes and utilities
- **Meaningful Names**: Descriptive variable and function names
- **Small Functions**: Functions focused on single tasks
- **Constants**: Magic numbers/strings replaced with named constants

### Modular Architecture

- **Base Scraper**: Common functionality for all scrapers
- **Specialized Scrapers**: Company and market-specific implementations
- **Core Utilities**: Reusable components for logging, file management, and data processing
- **Configuration**: Centralized constants and settings

### Error Handling

- Consistent error handling across all components
- Comprehensive logging with different levels
- Graceful degradation when data sources are unavailable

## ğŸ”§ Usage Examples

### Scrape Individual Company Data

```python
from scrapers import CompanyScraper

# Create scraper for Apple stock
scraper = CompanyScraper("AAPL")

# Fetch and save data
success = scraper.save_company_data("2020-01-01")
```

### Scrape Market Data

```python
from scrapers import MarketScraper

# Create market data scraper
scraper = MarketScraper(fred_api_key="your_key")

# Fetch and save market data
success = scraper.save_market_data("2020-01-01")
```

### Update Existing Data

```python
from update_all_refactored import DataUpdater

# Update all existing data
updater = DataUpdater(fred_api_key="your_key")
updater.update_all_data()
```

## ğŸ› ï¸ Development Tools

### Code Formatting

```bash
# Format code with black
black .

# Sort imports with isort
isort .
```

### Linting

```bash
# Check code style with flake8
flake8 .
```

## ğŸ“ˆ Data Sources

- **Stock Data**: Yahoo Finance (via yfinance)
- **Economic Data**: Federal Reserve Economic Data (FRED)
- **Market Indexes**: S&P 500, NASDAQ, VIX

## ğŸ—‚ï¸ Output Structure

Data is organized in timestamped folders:

```
scraped_data/
â”œâ”€â”€ AAPL_20200103_20250620/
â”‚   â””â”€â”€ AAPL_data.csv
â”œâ”€â”€ market_data_20200103_20250620/
â”‚   â””â”€â”€ market_data.csv
â””â”€â”€ ...
```

## ğŸ”„ Key Improvements from Original

1. **Eliminated Code Duplication**: Common functionality moved to base classes
2. **Better Error Handling**: Consistent exception handling with proper logging
3. **Cleaner Data Processing**: Centralized data cleaning logic
4. **Modular Design**: Easy to extend with new data sources
5. **Configuration Management**: Constants separated from logic
6. **Type Hints**: Better code documentation and IDE support
7. **Environment Variables**: Secure API key management
8. **Standardized Logging**: Consistent logging across all components

## ğŸ“ Configuration

Key settings in `constants.py`:

- `DEFAULT_TICKERS`: List of stock symbols to scrape
- `FRED_SERIES`: Economic indicators to fetch
- `MARKET_INDEXES`: Market indexes to track
- `OUTPUT_DIR`: Where to save data files

## ğŸ§ª Testing

The modular design makes it easy to add unit tests:

```python
# Example test structure
tests/
â”œâ”€â”€ test_company_scraper.py
â”œâ”€â”€ test_market_scraper.py
â”œâ”€â”€ test_data_processor.py
â””â”€â”€ test_file_manager.py
```

## ğŸ”® Future Enhancements

- Add unit tests for all components
- Implement data validation schemas
- Add support for more data sources
- Create web dashboard for monitoring
- Add data quality checks
- Implement retry mechanisms for failed requests
